{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-transformers in e:\\program files (x86)\\conda\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: fastprogress in e:\\program files (x86)\\conda\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: sentencepiece in e:\\program files (x86)\\conda\\lib\\site-packages (from pytorch-transformers) (0.1.97)\n",
      "Requirement already satisfied: sacremoses in e:\\program files (x86)\\conda\\lib\\site-packages (from pytorch-transformers) (0.0.53)\n",
      "Requirement already satisfied: requests in e:\\program files (x86)\\conda\\lib\\site-packages (from pytorch-transformers) (2.27.1)\n",
      "Requirement already satisfied: numpy in e:\\program files (x86)\\conda\\lib\\site-packages (from pytorch-transformers) (1.21.5)\n",
      "Requirement already satisfied: regex in e:\\program files (x86)\\conda\\lib\\site-packages (from pytorch-transformers) (2022.7.9)\n",
      "Requirement already satisfied: boto3 in e:\\program files (x86)\\conda\\lib\\site-packages (from pytorch-transformers) (1.16.63)\n",
      "Requirement already satisfied: torch>=1.0.0 in e:\\program files (x86)\\conda\\lib\\site-packages (from pytorch-transformers) (1.13.1)\n",
      "Requirement already satisfied: tqdm in e:\\program files (x86)\\conda\\lib\\site-packages (from pytorch-transformers) (4.64.1)\n",
      "Requirement already satisfied: typing_extensions in e:\\program files (x86)\\conda\\lib\\site-packages (from torch>=1.0.0->pytorch-transformers) (4.3.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in e:\\program files (x86)\\conda\\lib\\site-packages (from boto3->pytorch-transformers) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.63 in e:\\program files (x86)\\conda\\lib\\site-packages (from boto3->pytorch-transformers) (1.19.63)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in e:\\program files (x86)\\conda\\lib\\site-packages (from boto3->pytorch-transformers) (0.3.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in e:\\program files (x86)\\conda\\lib\\site-packages (from requests->pytorch-transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\program files (x86)\\conda\\lib\\site-packages (from requests->pytorch-transformers) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\program files (x86)\\conda\\lib\\site-packages (from requests->pytorch-transformers) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\program files (x86)\\conda\\lib\\site-packages (from requests->pytorch-transformers) (3.3)\n",
      "Requirement already satisfied: six in e:\\program files (x86)\\conda\\lib\\site-packages (from sacremoses->pytorch-transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in e:\\program files (x86)\\conda\\lib\\site-packages (from sacremoses->pytorch-transformers) (1.1.0)\n",
      "Requirement already satisfied: click in e:\\program files (x86)\\conda\\lib\\site-packages (from sacremoses->pytorch-transformers) (8.0.4)\n",
      "Requirement already satisfied: colorama in e:\\program files (x86)\\conda\\lib\\site-packages (from tqdm->pytorch-transformers) (0.4.5)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in e:\\program files (x86)\\conda\\lib\\site-packages (from botocore<1.20.0,>=1.19.63->boto3->pytorch-transformers) (2.8.1)\n"
     ]
    }
   ],
   "source": [
    "#!pip install pytorch-transformers fastprogress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "from enum import Enum\n",
    "from fastai import *\n",
    "from fastai.text import *\n",
    "from fastai.callback import *\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "from pytorch_transformers import BertTokenizer, BertPreTrainedModel, BertModel, BertConfig\n",
    "from pytorch_transformers import AdamW\n",
    "\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from datetime import datetime"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    },
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Check, if and what kind of GPU is used\n",
    "def get_memory_usage():\n",
    "    return torch.cuda.memory_allocated(device)/1000000\n",
    "\n",
    "def get_memory_usage_str():\n",
    "    return 'Memory usage: {:.2f} MB'.format(get_memory_usage())\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "if cuda_available:\n",
    "    curr_device = torch.cuda.current_device()\n",
    "    print(torch.cuda.get_device_name(curr_device))\n",
    "device = torch.device(\"cuda\" if cuda_available else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "    def set(self, key, val):\n",
    "        self[key] = val\n",
    "        setattr(self, key, val)\n",
    "\n",
    "class Fold(Enum):\n",
    "  No = 1\n",
    "  TenFold = 2\n",
    "  ProjFold = 3\n",
    "\n",
    "class Sampling(Enum):\n",
    "  NoSampling = 1\n",
    "  UnderSampling = 2\n",
    "  OverSampling = 3\n",
    "\n",
    "config = Config(\n",
    "    num_labels = 2, # will be set automatically afterwards\n",
    "    model_name=\"bert-base-cased\", # bert_base_uncased, bert_large_cased, bert_large_uncased\n",
    "    max_lr=2e-5, # default: 2e-5\n",
    "    moms=(0.8, 0.7), # default: (0.8, 0.7); alt.(0.95, 0.85)\n",
    "    epochs=16, # 10, 16, 32, 50\n",
    "    bs=16, # default: 16\n",
    "    weight_decay = 0.01,\n",
    "    max_seq_len=128, # 50, 128\n",
    "    train_size=0.75, # 0.8\n",
    "    loss_func=nn.CrossEntropyLoss(),\n",
    "    seed=904727489, #default: 904727489, 42 (as in Dalpiaz) or None\n",
    "    es = False, # True\n",
    "    min_delta = 0.01,\n",
    "    patience = 3,\n",
    "    fold = Fold.No, # Fold.No, Fold.TenFold, Fold.ProjFold\n",
    "    sampling = Sampling.NoSampling, #Sampling.UnderSampling, Sampling.NoSampling, Sampling.OverSampling\n",
    ")\n",
    "\n",
    "clazz = 'all' # 'Function', 'Data', 'Behavior' depending on selection in classes\n",
    "\n",
    "config_data = Config(\n",
    "    root_folder = '.', # where is the root folder? Keep it that way if you want to load from Google Drive\n",
    "    data_folder = '/', # where is the folder containing the datasets; relative to root\n",
    "    train_data = ['func_concerns.csv'], # dataset file to use\n",
    "    label_column = clazz,\n",
    "    log_folder_name = '/log/',\n",
    "    log_file = clazz + '_' + Fold(config.fold).name + '_' + Sampling(config.sampling).name + '_classifierPredictions_' + datetime.now().strftime('%Y%m%d-%H%M') + '.txt', # log-file name (make sure log folder exists)\n",
    "    result_file = clazz + '_' + Fold(config.fold).name + '_' + Sampling(config.sampling).name + '_classifierResults_' + datetime.now().strftime('%Y%m%d-%H%M') + '.txt', # result-file name (make sure log folder exists)\n",
    "    model_path = '/models/', # where is the folder for the model(s); relative to the root\n",
    "    model_name = 'NoRBERT.pkl', # what is the model name?\n",
    "    gdrive_root_folder = '/content/drive/My Drive/Code/Task1_to_3_original_Promise_NFR_dataset/', # Set this to the Google Drive path. Starts with '/content/drive/' and then usually 'My Drive/*' for the files in your Drive\n",
    "\n",
    "    orig_data_set_zip = 'https://zenodo.org/record/5541679/files/NoRBERT_RE20_Paper65.zip', # link to the data set (on zenodo). DO NOT CHANGE!\n",
    "    orig_data_zip_name = 'NoRBERT_RE20_Paper65.zip', # DO NOT CHANGE\n",
    "    orig_data_file_in_zip = 'Code/Task5_func_concerns_dataset/func_concerns.csv', # DO NOT CHANGE\n",
    "\n",
    "    # Project split to use, either p-fold (as in Dalpiaz) or loPo\n",
    "    #project_fold = [[3, 9, 11], [1, 5, 12], [6, 10, 13], [1, 8, 14], [3, 12, 15], [2, 5, 11], [6, 9, 14], [7, 8, 13], [2, 4, 15], [4, 7, 10] ], # p-fold\n",
    "    project_fold = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15] ], # loPo\n",
    "    classes = ['Function', 'Data', 'Behavior'],\n",
    "\n",
    ")\n",
    "\n",
    "load_from_gdrive = False # True, if you want to use Google Drive; else, False\n",
    "save_model = False # True, if you want to use save the model file (make sure model folder exists)\n",
    "input_col = 'RequirementText'\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Prepare data loading: Init loading from Google Drive, if set in config above. Else, download the data set from zenodo (using wget) {display-mode: \"form\"}\n",
    "if load_from_gdrive:\n",
    "    from google.colab import drive\n",
    "    # Connect to drive to load the corpus from there\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    config_data.root_folder = config_data.gdrive_root_folder\n",
    "else:\n",
    "    # If the file does not exist already, download the zip and extract the needed file\n",
    "    data_path = config_data.root_folder + config_data.data_folder + config_data.train_data[0]\n",
    "    data_file = Path(data_path)\n",
    "    if not data_file.exists():\n",
    "        !wget {config_data.orig_data_set_zip}\n",
    "        import zipfile\n",
    "        with zipfile.ZipFile(config_data.orig_data_zip_name) as z:\n",
    "            with open(data_path, 'wb') as f:\n",
    "                f.write(z.read(config_data.orig_data_file_in_zip))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Define logging functions and seed generation {display-mode: \"form\"}\n",
    "def initLog():\n",
    "    logfolder = config_data.root_folder + config_data.log_folder_name\n",
    "\n",
    "    if not os.path.isdir(logfolder):\n",
    "      print(\"Log folder does not exist, trying to create folder.\")\n",
    "      try:\n",
    "        os.mkdir(logfolder)\n",
    "      except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % logfolder)\n",
    "      else:\n",
    "        print (\"Successfully created the directory %s\" % logfolder)\n",
    "    logfile = logfolder + config_data.log_file\n",
    "    log_txt = datetime.now().strftime('%Y-%m-%d %H:%M') + ' ' + get_info()\n",
    "    with open(logfile, 'w') as log:\n",
    "        log.write(log_txt + '\\n')\n",
    "\n",
    "def logLine(line):\n",
    "    logfile = config_data.root_folder + config_data.log_folder_name  + config_data.log_file\n",
    "    with open(logfile, 'a') as log:\n",
    "        log.write(line + '\\n')\n",
    "\n",
    "def logResult(result):\n",
    "    logfile = config_data.root_folder + config_data.log_folder_name + config_data.result_file\n",
    "    with open(logfile, 'a') as log:\n",
    "        log.write(get_info() + '\\n')\n",
    "        log.write(result + '\\n')\n",
    "\n",
    "def get_info():\n",
    "     model_config = 'model: {}, max_lr: {}, epochs: {}, bs: {}, train_size: {}, weight decay: {},  Seed: {}, Data: {}, Column: {}, EarlyStopping: {}:{};pat:{}'.format(config.model_name, config.max_lr, config.epochs, config.bs, config.train_size, config.weight_decay, config.seed, config_data.train_data, config_data.label_column, config.es, config.min_delta, config.patience)\n",
    "     return model_config\n",
    "\n",
    "def set_seed(seed):\n",
    "    if seed is None:\n",
    "        seed = random.randint(0, 2**31)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    return seed\n",
    "\n",
    "set_seed(config.seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Create proper tokenizer for our data (adapting FastAiTokenizer to use BertTokenizer) {display-mode: \"form\"}\n",
    "class FastAiBertTokenizer(BaseTokenizer):\n",
    "    \"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\n",
    "    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=512, **kwargs):\n",
    "        self._pretrained_tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self\n",
    "\n",
    "    def tokenizer(self, t:str):\n",
    "        \"\"\"Limits the maximum sequence length. Prepend with [CLS] and append [SEP]\"\"\"\n",
    "        return [\"[CLS]\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"[SEP]\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Define Processors and Databunch {display-mode: \"form\"}\n",
    "class BertTokenizeProcessor(TokenizeProcessor):\n",
    "    \"\"\"Special Tokenizer, where we remove sos/eos tokens since we add that ourselves in the tokenizer.\"\"\"\n",
    "    def __init__(self, tokenizer):\n",
    "        super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False)\n",
    "\n",
    "class BertNumericalizeProcessor(NumericalizeProcessor):\n",
    "    \"\"\"Use a custom vocabulary to match the original BERT model.\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, vocab=Vocab(list(bert_tok.vocab.keys())), **kwargs)\n",
    "\n",
    "def get_bert_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n",
    "    return [BertTokenizeProcessor(tokenizer=tokenizer),\n",
    "            NumericalizeProcessor(vocab=vocab)]\n",
    "\n",
    "class BertDataBunch(TextDataBunch):\n",
    "    @classmethod\n",
    "    def from_df(cls, path:PathOrStr, train_df:DataFrame, valid_df:DataFrame, test_df:Optional[DataFrame]=None,\n",
    "              tokenizer:Tokenizer=None, vocab:Vocab=None, classes:Collection[str]=None, text_cols:IntsOrStrs=1,\n",
    "              label_cols:IntsOrStrs=0, **kwargs) -> DataBunch:\n",
    "        \"Create a `TextDataBunch` from DataFrames.\"\n",
    "        p_kwargs, kwargs = split_kwargs_by_func(kwargs, get_bert_processor)\n",
    "        # use our custom processors while taking tokenizer and vocab as kwargs\n",
    "        processor = get_bert_processor(tokenizer=tokenizer, vocab=vocab, **p_kwargs)\n",
    "        if classes is None and is_listy(label_cols) and len(label_cols) > 1: classes = label_cols\n",
    "        src = ItemLists(path, TextList.from_df(train_df, path, cols=text_cols, processor=processor),\n",
    "                      TextList.from_df(valid_df, path, cols=text_cols, processor=processor))\n",
    "        src = src.label_for_lm() if cls==TextLMDataBunch else src.label_from_df(cols=label_cols, classes=classes)\n",
    "        if test_df is not None: src.add_test(TextList.from_df(test_df, path, cols=text_cols))\n",
    "        return src.databunch(**kwargs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Define own BertTextClassifier class{display-mode: \"form\"}\n",
    "class BertTextClassifier(BertPreTrainedModel):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        config = BertConfig.from_pretrained(model_name)\n",
    "        super(BertTextClassifier, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(model_name, config=config)\n",
    "\n",
    "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(self.config.hidden_size, num_labels)\n",
    "\n",
    "\n",
    "    def forward(self, tokens, labels=None, position_ids=None, token_type_ids=None, attention_mask=None, head_mask=None):\n",
    "        outputs = self.bert(tokens, position_ids=position_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, head_mask=head_mask)\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(dropout_output)\n",
    "\n",
    "        activation = nn.Softmax(dim=1)\n",
    "        probs = activation(logits)\n",
    "\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Define functions to load data {display-mode: \"form\"}\n",
    "def load_data(filename):\n",
    "    fpath = config_data.root_folder + config_data.data_folder + filename\n",
    "    print(fpath)\n",
    "    df = pd.read_csv(fpath, delimiter=';', header=0, encoding='utf8', names=['ProjectID', 'RequirementText', 'Function', 'Data', 'Behavior',])\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "def load_all_data(filenames, label_column):\n",
    "    df = load_data(filenames[0])\n",
    "    for i in range(1, len(filenames)):\n",
    "        df = df.append(load_data(filenames[i]))\n",
    "\n",
    "    # shuffle the dataset a bit and get the amount of classes\n",
    "    df = df.sample(frac=1, axis=0, random_state = config.seed)\n",
    "    config.num_labels = df[label_column].nunique()\n",
    "\n",
    "    print(df.shape)\n",
    "    print(df[label_column].value_counts())\n",
    "    return df\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Create the dictionary that contains the labels along with their indices. This is useful for evaluation and similar. {display-mode: \"form\"}\n",
    "def create_label_indices(df):\n",
    "    #prepare label\n",
    "    labels = ['not_' + config_data.label_column, config_data.label_column]\n",
    "\n",
    "    #create dict\n",
    "    labelDict = dict()\n",
    "    for i in range (0, len(labels)):\n",
    "        labelDict[i] = labels[i]\n",
    "    return labelDict\n",
    "\n",
    "#label_indices = create_label_indices(df)\n",
    "#print(label_indices)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Define functions for under-/oversample dataset {display-mode: \"form\"}\n",
    "def undersample(df_trn, major_label, minor_label):\n",
    "  sample_size = sum(df_trn[config_data.label_column] == minor_label)\n",
    "  majority_indices = df_trn[df_trn[config_data.label_column] == major_label].index\n",
    "  random_indices = np.random.choice(majority_indices, sample_size, replace=False)\n",
    "  sample = df_trn.loc[random_indices]\n",
    "  sample = sample.append(df_trn[df_trn[config_data.label_column] == minor_label])\n",
    "  df_trn = sample\n",
    "  df_trn = df_trn.sample(frac=1, axis=0, random_state = config.seed)\n",
    "  print(df_trn[config_data.label_column].value_counts())\n",
    "  return df_trn\n",
    "\n",
    "def oversample(df_trn, major_label, minor_label):\n",
    "  minor_size = sum(df_trn[config_data.label_column] == minor_label)\n",
    "  major_size = sum(df_trn[config_data.label_column] == major_label)\n",
    "  multiplier = major_size//minor_size\n",
    "  sample = df_trn\n",
    "  minority_indices = df_trn[df_trn[config_data.label_column] == minor_label].index\n",
    "  diff = major_size - (multiplier * minor_size)\n",
    "  random_indices = np.random.choice(minority_indices, diff, replace=False)\n",
    "  sample = pd.concat([df_trn.loc[random_indices], sample], ignore_index=True)\n",
    "  for i in range(multiplier - 1):\n",
    "    sample = pd.concat([sample, df_trn[df_trn[config_data.label_column] == minor_label]], ignore_index=True)\n",
    "  df_trn = sample\n",
    "  df_trn = df_trn.sample(frac=1, axis=0, random_state = config.seed)\n",
    "  print(df_trn[config_data.label_column].value_counts())\n",
    "  return df_trn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Function to split dataframe according to Sampling strategy and train size {display-mode: \"form\"}\n",
    "def split_dataframe(df, train_size = 0.8, random_state = None):\n",
    "    # split data into training and validation set\n",
    "    df_trn, df_valid = train_test_split(df, stratify = df[config_data.label_column], train_size = train_size, random_state = random_state)\n",
    "    # apply sample strategy\n",
    "    sizeOne = sum(df_trn[config_data.label_column] == 1)\n",
    "    sizeZero = sum(df_trn[config_data.label_column] == 0)\n",
    "    major_label = 0\n",
    "    minor_label = 1\n",
    "    if sizeOne > sizeZero:\n",
    "      major_label = 1\n",
    "      minor_label = 0\n",
    "    if config.sampling == Sampling.UnderSampling:\n",
    "      df_trn = undersample(df_trn, major_label, minor_label)\n",
    "    elif config.sampling == Sampling.OverSampling:\n",
    "      df_trn = oversample(df_trn, major_label, minor_label)\n",
    "    return df_trn, df_valid"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Create a predictor class{display-mode: \"form\"}\n",
    "class Predictor:\n",
    "    def __init__(self, classifier):\n",
    "        self.classifier = classifier\n",
    "        self.classes = self.classifier.data.classes\n",
    "\n",
    "    def predict(self, text):\n",
    "        prediction = self.classifier.predict(text)\n",
    "        prediction_class = prediction[1]\n",
    "        return self.classes[prediction_class]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Define functions to create databunch, learner and actual classifier{display-mode: \"form\"}\n",
    "def create_databunch(config, df_trn, df_valid):\n",
    "    bert_tok = BertTokenizer.from_pretrained(config.model_name,)\n",
    "    fastai_tokenizer = Tokenizer(tok_func=FastAiBertTokenizer(bert_tok, max_seq_len=config.max_seq_len), pre_rules=[], post_rules=[])\n",
    "    fastai_bert_vocab = Vocab(list(bert_tok.vocab.keys()))\n",
    "    return BertDataBunch.from_df(\".\",\n",
    "                   train_df=df_trn,\n",
    "                   valid_df=df_valid,\n",
    "                   tokenizer=fastai_tokenizer,\n",
    "                   vocab=fastai_bert_vocab,\n",
    "                   bs=config.bs,\n",
    "                   text_cols=input_col,\n",
    "                   label_cols=config_data.label_column,\n",
    "                   collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n",
    "              )\n",
    "\n",
    "\n",
    "def create_learner(config, databunch):\n",
    "    model = BertTextClassifier(config.model_name, config.num_labels)\n",
    "\n",
    "    optimizer = partial(AdamW)\n",
    "    if config.es:\n",
    "      learner = Learner(\n",
    "        databunch, model,\n",
    "        optimizer,\n",
    "        wd = config.weight_decay,\n",
    "        metrics=FBeta(beta=1), #accuracy, (metric to optimize on)\n",
    "        loss_func=config.loss_func, callback_fns=[partial(EarlyStoppingCallback, monitor='f_beta', min_delta=config.min_delta, patience=config.patience)]\n",
    "      )\n",
    "    else:\n",
    "      learner = Learner(\n",
    "        databunch, model,\n",
    "        optimizer,\n",
    "        wd = config.weight_decay,\n",
    "        metrics=FBeta(beta=1), #accuracy, (metric to optimize on)\n",
    "        loss_func=config.loss_func,\n",
    "      )\n",
    "\n",
    "    return learner\n",
    "\n",
    "# Create the classifier\n",
    "def create_classifier(config, df):\n",
    "  df_trn, df_valid = split_dataframe(df, train_size = config.train_size, random_state = config.seed)\n",
    "  databunch = create_databunch(config, df_trn, df_valid)\n",
    "\n",
    "  return create_learner(config, databunch)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Define predict loop {display-mode: \"form\"}\n",
    "def predict_and_log_result(classifier, df_eval):\n",
    "  predictor = Predictor(classifier)\n",
    "  flat_predictions, flat_true_labels = [], []\n",
    "  column_index = df_eval.columns.get_loc(config_data.label_column)\n",
    "  for row in progress_bar(df_eval.itertuples(), total=len(df_eval)):\n",
    "      class_text = row.RequirementText\n",
    "      class_label = row[column_index+1]\n",
    "      flat_true_labels.append(class_label)\n",
    "      prediction = predictor.predict(class_text)\n",
    "      flat_predictions.append(prediction)\n",
    "\n",
    "      log_text = 'PID: {}, {}, {} -> {}'.format(row.ProjectID, class_text, label_indices.get(class_label), label_indices.get(prediction))\n",
    "      logLine(log_text)\n",
    "\n",
    "  # get labels in correct order\n",
    "  target_names = []\n",
    "  test_labels = unique_labels(flat_true_labels, flat_predictions)\n",
    "  test_labels = np.sort(test_labels)\n",
    "  for x in test_labels:\n",
    "    target_names.append(label_indices.get(x))\n",
    "\n",
    "  result = classification_report(flat_true_labels, flat_predictions, target_names=target_names, digits = 5)\n",
    "  logResult(result)\n",
    "  print(result)\n",
    "  return flat_predictions, flat_true_labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Define train and test loop{display-mode: \"form\"}\n",
    "def train_and_predict(df_train, df_eval, overall_flat_predictions, overall_flat_true_labels, results):\n",
    "  classifier = create_classifier(config, df_train)\n",
    "  # Train the classifier on train set\n",
    "  print(classifier.fit_one_cycle(config.epochs, max_lr=config.max_lr, moms=config.moms, wd=config.weight_decay))\n",
    "  #Predict on test set\n",
    "  flat_predictions, flat_true_labels = predict_and_log_result(classifier, df_eval)\n",
    "  overall_flat_predictions.extend(flat_predictions)\n",
    "  overall_flat_true_labels.extend(flat_true_labels)\n",
    "  test_labels = df_eval[config_data.label_column].unique()\n",
    "  test_labels = np.sort(test_labels)\n",
    "  results.extend(precision_recall_fscore_support(flat_true_labels, flat_predictions, labels = test_labels))\n",
    "  return classifier, overall_flat_predictions, overall_flat_true_labels, results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Define function to calculate averaged metric results {display-mode: \"form\"}\n",
    "def calcAverageMetrics(results):\n",
    "  precisions, recalls, fscores = [], [], []\n",
    "  for i in range(int(len(results)/4)):\n",
    "    precisions.append(results[i*4])\n",
    "    recalls.append(results[i*4+1])\n",
    "    fscores.append(results[i*4+2])\n",
    "  precision = [0]*len(precisions[0])\n",
    "  recall = [0]*len(recalls[0])\n",
    "  fscore = [0]*len(fscores[0])\n",
    "  for i in range(len(precisions)):\n",
    "    precision = precision + precisions[i]\n",
    "    recall = recall + recalls[i]\n",
    "    fscore = fscore + fscores[i]\n",
    "  precision = precision / int(len(results)/4)\n",
    "  recall = recall / int(len(results)/4)\n",
    "  fscore = fscore / int(len(results)/4)\n",
    "  return precision, recall, fscore"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Decide how to fold and train the classifier {display-mode: \"form\"}\n",
    "# run train/eval loop for each in classes defined class subsequently\n",
    "for cl in config_data.classes:\n",
    "  config_data.label_column = cl\n",
    "  # load the train dataset\n",
    "  df = load_all_data(config_data.train_data, cl)\n",
    "  label_indices = create_label_indices(df)\n",
    "  print(label_indices)\n",
    "\n",
    "  overall_flat_predictions, overall_flat_true_labels, results = [], [], []\n",
    "  initLog()\n",
    "  if config.fold == Fold.TenFold:\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    fold_number = 1\n",
    "    for train, test in skf.split(df, df[config_data.label_column]):\n",
    "      df_train = df.iloc[train]\n",
    "      df_eval = df.iloc[test]\n",
    "      log_text = '/////////////////////// Fold: {} of {} /////////////////////////////'.format(fold_number,10)\n",
    "      logLine(log_text)\n",
    "      classifier, overall_flat_predictions, overall_flat_true_labels, results = train_and_predict(df_train, df_eval, overall_flat_predictions, overall_flat_true_labels, results)\n",
    "      fold_number = fold_number + 1\n",
    "  elif config.fold == Fold.ProjFold:\n",
    "    for k in config_data.project_fold:\n",
    "      test = df.loc[df['ProjectID'].isin(k)].index\n",
    "      train = df.loc[~df['ProjectID'].isin(k)].index\n",
    "      df_train = df.loc[train]\n",
    "      df_eval = df.loc[test]\n",
    "      log_text = '/////////////////////// Test-Projects: {} /////////////////////////////'.format(k)\n",
    "      logLine(log_text)\n",
    "      classifier, overall_flat_predictions, overall_flat_true_labels, results = train_and_predict(df_train, df_eval, overall_flat_predictions, overall_flat_true_labels, results)\n",
    "  else:\n",
    "    df_train, df_eval = train_test_split(df,stratify=df[config_data.label_column], train_size=config.train_size, random_state= config.seed)\n",
    "    classifier, overall_flat_predictions, overall_flat_true_labels, results = train_and_predict(df_train, df_eval, overall_flat_predictions, overall_flat_true_labels, results)\n",
    "\n",
    "  target_names = []\n",
    "  test_labels = df_eval[config_data.label_column].unique()\n",
    "\n",
    "  test_labels = np.sort(test_labels)\n",
    "  for x in test_labels:\n",
    "      target_names.append(label_indices.get(x))\n",
    "\n",
    "  print('/////////////////////// Aggregated Predictions Result /////////////////////////////')\n",
    "  logResult('/////////////////////// Aggregated Predictions Result /////////////////////////////')\n",
    "  result = classification_report(overall_flat_true_labels, overall_flat_predictions, target_names=target_names, digits = 5)\n",
    "  logResult(result)\n",
    "  print(result)\n",
    "  print('/////////////////////// Averaged Metrics Result /////////////////////////////')\n",
    "  logResult('/////////////////////// Averaged Metrics Result /////////////////////////////')\n",
    "  precision, recall, fscore = calcAverageMetrics(results)\n",
    "  print(\"              precision    recall  f1-score\")\n",
    "  logResult(\"              precision    recall  f1-score\")\n",
    "  for i in range(len(precision)):\n",
    "    print('{:<14}'.format(target_names[i]) + '  {:.5f}'.format(precision[i]) + '   {:.5f}'.format(recall[i]) + '   {:.5f}'.format(fscore[i]))\n",
    "    logResult('{:<14}'.format(target_names[i]) + '  {:.5f}'.format(precision[i]) + '   {:.5f}'.format(recall[i]) + '   {:.5f}'.format(fscore[i]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Save the model along with its config\n",
    "def create_model_name():\n",
    "    name = 'NoRBERT_{clasz}_e{epochs}_{sampling}'.format(clasz=clazz, epochs=str(config.epochs),sampling=Sampling(config.sampling).name)\n",
    "    return name\n",
    "\n",
    "def save_config(model_save_path, model_name):\n",
    "    settings = ''\n",
    "    for item in config.__dict__:\n",
    "        value = config[item]\n",
    "        setting = '{item}={value},\\n'.format(item=item, value=value)\n",
    "        settings += setting\n",
    "    save_path = model_save_path + model_name + '.config'\n",
    "    with open(save_path, 'w', encoding='utf-8') as out:\n",
    "        out.write(settings)\n",
    "\n",
    "if save_model:\n",
    "  model_name = create_model_name()\n",
    "  model_save_path = config_data.root_folder + config_data.model_path\n",
    "  if not os.path.isdir(model_save_path):\n",
    "    print(\"Models folder does not exist, trying to create folder.\")\n",
    "    try:\n",
    "      os.mkdir(model_save_path)\n",
    "    except OSError:\n",
    "      print (\"Creation of the directory %s failed\" % model_save_path)\n",
    "    else:\n",
    "      print (\"Successfully created the directory %s\" % model_save_path)\n",
    "  save_config(model_save_path, model_name)\n",
    "  model_save_file = model_save_path + model_name + '.pkl'\n",
    "  classifier.export(file = model_save_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
