{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install argparse,logging,os,random,sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils.utils_metrics import get_entities_bio, f1_score, classification_report\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "from models.model_ner import (\n",
    "    MODEL_FOR_SOFTMAX_NER_MAPPING,\n",
    "    MODEL_PRETRAINED_CONFIG_ARCHIVE_MAPPING,\n",
    "    AutoModelForSoftmaxNer,\n",
    ")\n",
    "\n",
    "from utils.utils_ner import convert_examples_to_features, get_labels, read_examples_from_file, collate_fn\n",
    "from utils.utils_adversarial import FGM, PGD\n",
    "\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_SOFTMAX_NER_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
    "MODEL_MAPS = tuple\n",
    "ALL_MODELS = sum((tuple(MODEL_PRETRAINED_CONFIG_ARCHIVE_MAPPING[conf].keys()) for conf in MODEL_CONFIG_CLASSES), ())\n",
    "TOKENIZER_ARGS = [\"do_lower_case\", \"strip_accents\", \"keep_accents\", \"use_fast\"]\n",
    "\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "def train(args, train_dataset, model, tokenizer, labels, pad_token_label_id):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer = SummaryWriter(args.output_dir)\n",
    "\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                  sampler=train_sampler,\n",
    "                                  batch_size=args.train_batch_size,\n",
    "                                  collate_fn=collate_fn)\n",
    "\n",
    "    if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    args.logging_steps = eval(args.logging_steps)\n",
    "    if isinstance(args.logging_steps, float):\n",
    "        args.logging_steps = int(args.logging_steps * len(train_dataloader)) // args.gradient_accumulation_steps\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    bert_parameters = eval('model.{}'.format(args.model_type)).named_parameters()\n",
    "    classifier_parameters = model.classifier.named_parameters()\n",
    "    args.bert_lr = args.bert_lr if args.bert_lr else args.learning_rate\n",
    "    args.classifier_lr = args.classifier_lr if args.classifier_lr else args.learning_rate\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in bert_parameters if not any(nd in n for nd in no_decay)],\n",
    "         \"weight_decay\": args.weight_decay,\n",
    "         \"lr\": args.bert_lr},\n",
    "        {\"params\": [p for n, p in bert_parameters if any(nd in n for nd in no_decay)],\n",
    "         \"weight_decay\": 0.0,\n",
    "         \"lr\": args.bert_lr},\n",
    "\n",
    "        {\"params\": [p for n, p in classifier_parameters if not any(nd in n for nd in no_decay)],\n",
    "         \"weight_decay\": args.weight_decay,\n",
    "         \"lr\": args.classifier_lr},\n",
    "        {\"params\": [p for n, p in classifier_parameters if any(nd in n for nd in no_decay)],\n",
    "         \"weight_decay\": 0.0,\n",
    "         \"lr\": args.classifier_lr}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    # Check if saved optimizer or scheduler states exist\n",
    "    if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n",
    "            os.path.join(args.model_name_or_path, \"scheduler.pt\")\n",
    "    ):\n",
    "        # Load in optimizer and scheduler states\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
    "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
    "\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "\n",
    "    # multi-gpu training (should be after apex fp16 initialization)\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Distributed training (should be after apex fp16 initialization)\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(\n",
    "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n",
    "        )\n",
    "\n",
    "    # adversarial_training\n",
    "    if args.adv_training == 'fgm':\n",
    "        adv = FGM(model=model, param_name='word_embeddings')\n",
    "    elif args.adv_training == 'pgd':\n",
    "        adv = PGD(model=model, param_name='word_embeddings')\n",
    "\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    logger.info(\n",
    "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "        args.train_batch_size\n",
    "        * args.gradient_accumulation_steps\n",
    "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
    "    )\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    epochs_trained = 0\n",
    "    best_score = 0.0\n",
    "    steps_trained_in_current_epoch = 0\n",
    "    # Check if continuing training from a checkpoint\n",
    "    if os.path.exists(args.model_name_or_path):\n",
    "        # set global_step to gobal_step of last saved checkpoint from model path\n",
    "        try:\n",
    "            global_step = int(args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0])\n",
    "        except ValueError:\n",
    "            global_step = 0\n",
    "        epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "        steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "\n",
    "        logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "        logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
    "        logger.info(\"  Continuing training from global step %d\", global_step)\n",
    "        logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
    "\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(\n",
    "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
    "    )\n",
    "    set_seed(args)  # Added here for reproductibility\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "\n",
    "            # Skip past any already trained steps if resuming training\n",
    "            if steps_trained_in_current_epoch > 0:\n",
    "                steps_trained_in_current_epoch -= 1\n",
    "                continue\n",
    "\n",
    "            model.train()\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "            inputs = {\"input_ids\": batch[0],\n",
    "                      \"attention_mask\": batch[1],\n",
    "                      \"valid_mask\": batch[2],\n",
    "                      \"labels\": batch[4], }\n",
    "            if args.model_type != \"distilbert\":\n",
    "                inputs[\"token_type_ids\"] = (\n",
    "                    batch[3] if args.model_type in [\"bert\", \"xlnet\"] else None\n",
    "                )  # XLM and RoBERTa don\"t use segment_ids\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0]  # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            if args.adv_training:\n",
    "                adv.adversarial_training(args, inputs, optimizer)\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            epoch_iterator.set_description('Loss: {}'.format(round(loss.item(), 6)))\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                if args.fp16:\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    # Log metrics\n",
    "                    if (\n",
    "                            args.local_rank == -1 and args.evaluate_during_training\n",
    "                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                        results, _ = evaluate(args, model, tokenizer, labels, pad_token_label_id, mode=\"dev\",\n",
    "                                              prefix=global_step)\n",
    "                        for key, value in results.items():\n",
    "                            if isinstance(value, float) or isinstance(value, int):\n",
    "                                tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
    "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                    if best_score < results['f1']:\n",
    "                        best_score = results['f1']\n",
    "                        output_dir = os.path.join(args.output_dir, \"best_checkpoint\")\n",
    "                        if not os.path.exists(output_dir):\n",
    "                            os.makedirs(output_dir)\n",
    "                        model_to_save = (\n",
    "                            model.module if hasattr(model, \"module\") else model\n",
    "                        )  # Take care of distributed/parallel training\n",
    "                        model_to_save.save_pretrained(output_dir)\n",
    "                        tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "                        torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "                        logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "                        torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "                        torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "                        logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer.close()\n",
    "\n",
    "    return global_step, tr_loss / global_step\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate(args, model, tokenizer, labels, pad_token_label_id, mode, prefix=\"\"):\n",
    "    eval_dataset = load_and_cache_examples(args, tokenizer, labels, pad_token_label_id, mode=mode)\n",
    "\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    # Note that DistributedSampler samples randomly\n",
    "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset,\n",
    "                                 sampler=eval_sampler,\n",
    "                                 batch_size=args.eval_batch_size)\n",
    "\n",
    "    # multi-gpu evaluate\n",
    "    if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation %s *****\", prefix)\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    trues = None\n",
    "    model.eval()\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\"input_ids\": batch[0],\n",
    "                      \"attention_mask\": batch[1],\n",
    "                      \"valid_mask\": batch[2],\n",
    "                      \"labels\": batch[4], }\n",
    "            if args.model_type != \"distilbert\":\n",
    "                inputs[\"token_type_ids\"] = (\n",
    "                    batch[2] if args.model_type in [\"bert\", \"xlnet\"] else None\n",
    "                )  # XLM and RoBERTa don\"t use segment_ids\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "            if args.n_gpu > 1:\n",
    "                tmp_eval_loss = tmp_eval_loss.mean()  # mean() to average on multi-gpu parallel evaluating\n",
    "            eval_loss += tmp_eval_loss.item()\n",
    "        nb_eval_steps += 1\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            trues = inputs[\"labels\"].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            trues = np.append(trues, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    preds = np.argmax(preds, axis=2)\n",
    "    label_map = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "    trues_list = [[] for _ in range(trues.shape[0])]\n",
    "    preds_list = [[] for _ in range(preds.shape[0])]\n",
    "\n",
    "    for i in range(trues.shape[0]):\n",
    "        for j in range(trues.shape[1]):\n",
    "            if trues[i, j] != pad_token_label_id:\n",
    "                trues_list[i].append(label_map[trues[i][j]])\n",
    "                preds_list[i].append(label_map[preds[i][j]])\n",
    "\n",
    "    true_entities = get_entities_bio(trues_list)\n",
    "    pred_entities = get_entities_bio(preds_list)\n",
    "    results = {\n",
    "        \"loss\": eval_loss,\n",
    "        \"f1\": f1_score(true_entities, pred_entities),\n",
    "        'report': classification_report(true_entities, pred_entities)\n",
    "    }\n",
    "\n",
    "    # 先打印训练阶段次数，后dev\n",
    "    '''\n",
    "        ***** Eval results 175 *****\n",
    "        ***** Eval loss : 0.517138389834002 *****\n",
    "        f1 = 0.9053721463124841\n",
    "        loss = 0.517138389834002\n",
    "        report =            precision    recall  f1-score   support\n",
    "\n",
    "              LOC    0.93889   0.91998   0.92934      1837\n",
    "              PER    0.96456   0.96037   0.96246      1842\n",
    "              ORG    0.89253   0.87323   0.88277      1341\n",
    "             MISC    0.78659   0.76356   0.77490       922\n",
    "\n",
    "        micro avg    0.91320   0.89768   0.90537      5942\n",
    "        macro avg    0.91275   0.89768   0.90513      5942\n",
    "    '''\n",
    "    output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)\n",
    "    with open(output_eval_file, \"a\") as writer:\n",
    "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "        writer.write(\"***** Eval results {} *****\\n\".format(prefix))\n",
    "        writer.write(\"***** Eval loss : {} *****\\n\".format(eval_loss))\n",
    "        for key in sorted(results.keys()):\n",
    "            if key == 'report_dict':\n",
    "                continue\n",
    "            logger.info(\"{} = {}\".format(key, str(results[key])))\n",
    "            writer.write(\"{} = {}\\n\".format(key, str(results[key])))\n",
    "    return results, preds_list\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 制作数据集\n",
    "def load_and_cache_examples(args, tokenizer, labels, pad_token_label_id, mode):\n",
    "    if args.local_rank not in [-1, 0] and not evaluate:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "\n",
    "    # Load data features from cache or dataset file\n",
    "    cached_features_file = os.path.join(\n",
    "        args.data_dir,\n",
    "        \"cached_{}_{}_{}\".format(\n",
    "            mode, list(filter(None, args.model_name_or_path.split(\"/\"))).pop(), str(args.max_seq_length)\n",
    "        ),\n",
    "    )\n",
    "    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "        features = torch.load(cached_features_file)\n",
    "    else:\n",
    "        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
    "        examples = read_examples_from_file(args.data_dir, mode)\n",
    "        features = convert_examples_to_features(\n",
    "            examples,\n",
    "            labels,\n",
    "            args.max_seq_length,\n",
    "            tokenizer,\n",
    "            cls_token_at_end=bool(args.model_type in [\"xlnet\"]),\n",
    "            # xlnet has a cls token at the end\n",
    "            cls_token=tokenizer.cls_token,\n",
    "            cls_token_segment_id=2 if args.model_type in [\"xlnet\"] else 0,\n",
    "            sep_token=tokenizer.sep_token,\n",
    "            sep_token_extra=bool(args.model_type in [\"roberta\"]),\n",
    "            # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n",
    "            pad_on_left=bool(args.model_type in [\"xlnet\"]),\n",
    "            # pad on the left for xlnet\n",
    "            pad_token=tokenizer.pad_token_id,\n",
    "            pad_token_segment_id=tokenizer.pad_token_type_id,\n",
    "            pad_token_label_id=pad_token_label_id,\n",
    "        )\n",
    "        if args.local_rank in [-1, 0]:\n",
    "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "            torch.save(features, cached_features_file)\n",
    "\n",
    "    if args.local_rank == 0 and not evaluate:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "\n",
    "    # Convert to Tensors and build dataset\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_valid_mask = torch.tensor([f.valid_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(all_input_ids, all_input_mask, all_valid_mask, all_segment_ids, all_label_ids)\n",
    "    return dataset\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Required parameters\n",
    "    parser.add_argument(\n",
    "        \"--data_dir\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"The input data dir. Should contain the training files for the CoNLL-2003 NER task.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_type\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Model type selected in the list: \" + \", \".join(MODEL_TYPES),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name_or_path\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Path to pre-trained model or shortcut name selected in the list: \" + \", \".join(ALL_MODELS),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
    "    )\n",
    "\n",
    "    # Other parameters\n",
    "    parser.add_argument(\n",
    "        \"--labels\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        help=\"Path to a file containing all labels. If not specified, CoNLL-2003 labels are used.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--tokenizer_name\",\n",
    "        default=\"\",\n",
    "        type=str,\n",
    "        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--cache_dir\",\n",
    "        default=\"\",\n",
    "        type=str,\n",
    "        help=\"Where do you want to store the pre-trained models downloaded from s3\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_seq_length\",\n",
    "        default=128,\n",
    "        type=int,\n",
    "        help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "             \"than this will be truncated, sequences shorter will be padded.\",\n",
    "    )\n",
    "    parser.add_argument(\"--do_train\", default=True, action=\"store_true\", help=\"Whether to run training.\")\n",
    "    parser.add_argument(\"--do_eval\", default=True, action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n",
    "    parser.add_argument(\"--do_predict\", action=\"store_true\", help=\"Whether to run predictions on the test set.\")\n",
    "    parser.add_argument(\n",
    "        \"--evaluate_during_training\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether to run evaluation during training at each logging step.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--do_lower_case\", action=\"store_true\", help=\"Set this flag if you are using an uncased model.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--keep_accents\", action=\"store_const\", const=True, help=\"Set this flag if model is trained with accents.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--strip_accents\", action=\"store_const\", const=True, help=\"Set this flag if model is trained without accents.\"\n",
    "    )\n",
    "    parser.add_argument(\"--use_fast\", action=\"store_const\", const=True, help=\"Set this flag to use fast tokenization.\")\n",
    "    parser.add_argument(\"--per_gpu_train_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for training.\")\n",
    "    parser.add_argument(\n",
    "        \"--per_gpu_eval_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for evaluation.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gradient_accumulation_steps\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
    "    )\n",
    "    parser.add_argument(\"--loss_type\", default=\"lsr\", type=str, help=\"The loss function to optimize.\")\n",
    "    parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "    parser.add_argument(\"--bert_lr\", type=float, help=\"The initial learning rate for BERT.\")\n",
    "    parser.add_argument(\"--classifier_lr\", type=float, help=\"The initial learning rate of classifier.\")\n",
    "    parser.add_argument(\"--adv_training\", default=None, choices=['fgm', 'pgd'], help=\"fgm adversarial training\")\n",
    "\n",
    "\n",
    "    parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
    "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
    "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
    "    parser.add_argument(\n",
    "        \"--num_train_epochs\", default=3.0, type=float, help=\"Total number of training epochs to perform.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_steps\",\n",
    "        default=-1,\n",
    "        type=int,\n",
    "        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n",
    "    )\n",
    "    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
    "\n",
    "    parser.add_argument(\"--logging_steps\", type=str, default='0.1', help=\"Log every X updates steps.\")\n",
    "    parser.add_argument(\n",
    "        \"--eval_all_checkpoints\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n",
    "    )\n",
    "    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
    "    parser.add_argument(\n",
    "        \"--overwrite_output_dir\", action=\"store_true\", help=\"Overwrite the content of the output directory\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\"\n",
    "    )\n",
    "    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--fp16\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fp16_opt_level\",\n",
    "        type=str,\n",
    "        default=\"O1\",\n",
    "        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "             \"See details at https://nvidia.github.io/apex/amp.html\",\n",
    "    )\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n",
    "    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n",
    "    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if (\n",
    "            os.path.exists(args.output_dir)\n",
    "            and os.listdir(args.output_dir)\n",
    "            and args.do_train\n",
    "            and not args.overwrite_output_dir\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "                args.output_dir\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Setup distant debugging if needed\n",
    "    if args.server_ip and args.server_port:\n",
    "        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
    "        import ptvsd\n",
    "\n",
    "        print(\"Waiting for debugger attach\")\n",
    "        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
    "        ptvsd.wait_for_attach()\n",
    "\n",
    "    # Setup CUDA, GPU & distributed training\n",
    "    if args.local_rank == -1 or args.no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n",
    "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.cuda.set_device(args.local_rank)\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        torch.distributed.init_process_group(backend=\"nccl\")\n",
    "        args.n_gpu = 1\n",
    "    args.device = device\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "        args.local_rank,\n",
    "        device,\n",
    "        args.n_gpu,\n",
    "        bool(args.local_rank != -1),\n",
    "        args.fp16,\n",
    "    )\n",
    "\n",
    "    # Set seed\n",
    "    set_seed(args)\n",
    "\n",
    "    # Prepare CONLL-2003 task\n",
    "    labels = get_labels(args.labels)\n",
    "    num_labels = len(labels)\n",
    "    # Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later\n",
    "    pad_token_label_id = CrossEntropyLoss().ignore_index\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    if args.local_rank not in [-1, 0]:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "\n",
    "    args.model_type = args.model_type.lower()\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        args.config_name if args.config_name else args.model_name_or_path,\n",
    "        num_labels=num_labels,\n",
    "        id2label={str(i): label for i, label in enumerate(labels)},\n",
    "        label2id={label: i for i, label in enumerate(labels)},\n",
    "        cache_dir=args.cache_dir if args.cache_dir else None,\n",
    "    )\n",
    "    #####\n",
    "    setattr(config, 'loss_type', args.loss_type)\n",
    "    #####\n",
    "    tokenizer_args = {k: v for k, v in vars(args).items() if v is not None and k in TOKENIZER_ARGS}\n",
    "    logger.info(\"Tokenizer arguments: %s\", tokenizer_args)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n",
    "        cache_dir=args.cache_dir if args.cache_dir else None,\n",
    "        **tokenizer_args,\n",
    "    )\n",
    "    model = AutoModelForSoftmaxNer.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "        config=config,\n",
    "        cache_dir=args.cache_dir if args.cache_dir else None,\n",
    "    )\n",
    "\n",
    "    if args.local_rank == 0:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "\n",
    "    model.to(args.device)\n",
    "\n",
    "    logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "    # Training\n",
    "    if args.do_train:\n",
    "        train_dataset = load_and_cache_examples(args, tokenizer, labels, pad_token_label_id, mode=\"train\")\n",
    "        global_step, tr_loss = train(args, train_dataset, model, tokenizer, labels, pad_token_label_id)\n",
    "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
    "        # Create output directory if needed\n",
    "        if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n",
    "            os.makedirs(args.output_dir)\n",
    "\n",
    "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "        # They can then be reloaded using `from_pretrained()`\n",
    "        model_to_save = (\n",
    "            model.module if hasattr(model, \"module\") else model)  # Take care of distributed/parallel training\n",
    "        model_to_save.save_pretrained(args.output_dir)\n",
    "        tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "        # Good practice: save your training arguments together with the trained model\n",
    "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
    "\n",
    "    # Evaluation\n",
    "    if args.do_eval and args.local_rank in [-1, 0]:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.output_dir, **tokenizer_args)\n",
    "        checkpoint = os.path.join(args.output_dir, 'best_checkpoint')\n",
    "        model = AutoModelForSoftmaxNer.from_pretrained(checkpoint)\n",
    "        model.to(args.device)\n",
    "        results, _ = evaluate(args, model, tokenizer, labels, pad_token_label_id, mode=\"dev\", prefix='dev')\n",
    "        output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
    "        with open(output_eval_file, \"a\") as writer:\n",
    "            writer.write('***** Predict in dev dataset *****\\n')\n",
    "            writer.write(\"{} = {}\\n\".format('report', str(results['report'])))\n",
    "\n",
    "    if args.do_predict and args.local_rank in [-1, 0]:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.output_dir, **tokenizer_args)\n",
    "        checkpoint = os.path.join(args.output_dir, 'best_checkpoint')\n",
    "        model = AutoModelForSoftmaxNer.from_pretrained(checkpoint)\n",
    "        model.to(args.device)\n",
    "        results, predictions = evaluate(args, model, tokenizer, labels, pad_token_label_id, mode=\"test\", prefix='test')\n",
    "        # Save results\n",
    "        output_test_results_file = os.path.join(args.output_dir, \"test_results.txt\")\n",
    "        with open(output_test_results_file, \"w\") as writer:\n",
    "            writer.write('***** Predict in dev dataset *****\\n')\n",
    "            writer.write(\"{} = {}\\n\".format('report', str(results['report'])))\n",
    "\n",
    "        # Save predictions\n",
    "        output_test_predictions_file = os.path.join(args.output_dir, \"test_predictions.txt\")\n",
    "        with open(output_test_predictions_file, \"w\") as writer:\n",
    "            with open(os.path.join(args.data_dir, \"test.txt\"), \"r\") as f:\n",
    "                example_id = 0\n",
    "                for line in f:\n",
    "                    if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "                        writer.write(line)\n",
    "                        if not predictions[example_id]:\n",
    "                            example_id += 1\n",
    "                    elif predictions[example_id]:\n",
    "                        output_line = line.split()[0] + \" \" + predictions[example_id].pop(0) + \"\\n\"\n",
    "                        writer.write(output_line)\n",
    "                    else:\n",
    "                        logger.warning(\"Maximum sequence length exceeded: No prediction for '%s'.\", line.split()[0])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "main()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
