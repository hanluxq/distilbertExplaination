{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HpCXxfYaJHeD"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers torch shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import shap\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data_path = 'textLabel.csv'\n",
    "labeled_path = 'output1.csv'\n",
    "promise_path = 'promise_nfr.csv'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data = pd.read_csv(promise_path,sep=';',usecols=[\"RequirementText\",\"NFR\"])"
   ],
   "metadata": {
    "id": "HhFJdDB_JJh9"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# 分割数据集\n",
    "train_text, val_text, train_labels, val_labels = train_test_split(data['RequirementText'], data['NFR'], test_size=0.2)\n",
    "train_text = train_text.iloc[:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**DistilBert**"
   ],
   "metadata": {
    "id": "jqpP0q5AjspJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\").cuda()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DZDnh98kRkep",
    "outputId": "983764b0-a997-49c2-c954-eea54497196e"
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# use the custom function\n",
    "import scipy as sp\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    使用模型预测输入文本的情感分数。\n",
    "\n",
    "    Args:\n",
    "    x: List[str] -- 包含多个文本的列表。\n",
    "\n",
    "    Returns:\n",
    "    List[float] -- 包含多个文本的情感分数。\n",
    "    \"\"\"\n",
    "    tv = torch.tensor([tokenizer.encode(text, padding='max_length', max_length=128, truncation=True) for text in x]).cuda()\n",
    "    attention_mask = (tv!=0).type(torch.int64).cuda()\n",
    "    outputs = model(tv,attention_mask=attention_mask)[0].detach().cpu().numpy()\n",
    "    scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T\n",
    "    val = sp.special.logit(scores)\n",
    "    return val"
   ],
   "metadata": {
    "id": "lC1TatRUCTLN"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "#计算重要特征，返回特征与重要性\n",
    "def important(shap_values):\n",
    "    \"\"\"\n",
    "    根据SHAP值确定每个样例中最重要的特征。\n",
    "    Args:\n",
    "    shap_values: List[shap.Explanation] -- 一个包含多个SHAP值解释的列表，每个SHAP值解释对应一个样例的所有特征的SHAP值。\n",
    "\n",
    "    Returns:\n",
    "    List[List[Tuple[str, float]]] -- 一个包含多个样例的列表，每个样例都是一个包含多个元组的列表，每个元组包含两个元素，一个是最重要的特征名，一个是该特征的SHAP值。\n",
    "    \"\"\"\n",
    "\n",
    "    reason = []\n",
    "    for ele in shap_values:\n",
    "        sum =0\n",
    "        for num in ele.values:\n",
    "            sum += abs(num[0])\n",
    "        avg = sum/len(ele.values)\n",
    "        res = []\n",
    "        for values,datas in zip(ele.values,ele.data):\n",
    "            cur = []\n",
    "            if abs(values[0])>=avg:\n",
    "                cur.append(datas)\n",
    "                cur.append(values[0])\n",
    "                res.append(cur)\n",
    "        reason.append(res)\n",
    "    return reason"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "7bk-cHksjy9l"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#读取csv文件\n",
    "def read_file(file_name):\n",
    "    \"\"\"\n",
    "    读取CSV文件，返回所有数据行的列表。\n",
    "    Args:\n",
    "    file_name: str -- 要读取的CSV文件路径及文件名。\n",
    "\n",
    "    Returns:\n",
    "    List[List[str]] -- 一个包含所有数据行的列表，每个数据行都是一个包含多个数据字段的字符串列表。\n",
    "    \"\"\"\n",
    "\n",
    "    csv_file = open(file_name, encoding=\"utf-8\")\n",
    "    csv_reader_lines = csv.reader(csv_file)\n",
    "    raw_date = []\n",
    "    for i, line in enumerate(csv_reader_lines):\n",
    "        raw_date.append(line)\n",
    "    return raw_date"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "#读取标注文件，返回关注点\n",
    "def readLabel(file_path):\n",
    "    \"\"\"\n",
    "    读取标注数据文件，提取每个文档的单词和对应的标签。\n",
    "\n",
    "    Args:\n",
    "    file_path: str -- 标注数据文件路径，应为CSV格式，每行为一个文档的标注结果，其中每个文档的标注结果以JSON格式保存。\n",
    "\n",
    "    Returns:\n",
    "    Tuple[List[List[str]], List[List[str]]] -- 一个元组，包含两个列表，第一个列表为每个文档的单词列表，第二个列表为每个文档的标签列表。\n",
    "    \"\"\"\n",
    "    data = read_file(file_path)\n",
    "    words = []\n",
    "    labels = []\n",
    "    for ele in data[1:]:\n",
    "        #正则表达式提取每个文档的单词和对应的标签\n",
    "        features = []\n",
    "        label = []\n",
    "        #数据从第二行开始\n",
    "        for e in ele:\n",
    "            match_text = re.search(r'\"text\": \"(.*)\"', e.strip())\n",
    "            match_label = re.search(r'\"labels\": \\[\"(\\w+)\"\\]',e.strip())\n",
    "            if match_text:\n",
    "                features.append(match_text.group(1).strip())\n",
    "            if match_label:\n",
    "                label.append(match_label.group(1).strip())\n",
    "        words.append(features)\n",
    "        labels.append(label)\n",
    "    return words,labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def shapValues(data):\n",
    "    \"\"\"\n",
    "    计算每个样例的SHAP值。\n",
    "    Args:\n",
    "    data: List[str] -- 一个包含多个文本样例的列表，每个样例都是一个字符串。\n",
    "\n",
    "    Returns:\n",
    "    Explanation -- 包含每个样例的SHAP值的对象。\n",
    "    \"\"\"\n",
    "\n",
    "    explainer = shap.Explainer(f,tokenizer,output_names=[\"FR\",\"NFR\"])\n",
    "    return explainer(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer: 51it [01:13,  1.52s/it]                        \n"
     ]
    }
   ],
   "source": [
    "concernsData = pd.read_csv(data_path,sep=',',usecols=[\"RequirementText\",\"Function\",\"Data\",\"Behavior\"])\n",
    "sample = concernsData['RequirementText']\n",
    "shap_values = shapValues(sample)\n",
    "importance = important(shap_values)\n",
    "words,labs = readLabel(labeled_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "评估特征"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "#检测异常特征\n",
    "def errCheck(importance,words):\n",
    "    \"\"\"\n",
    "    检查每个样例中是否有特征未出现在关注点中。\n",
    "\n",
    "    Args:\n",
    "    importance: List[Tuple[str, float]] -- 一个包含多个样例的列表，每个样例都是一个包含多个重要特征的元组列表，每个元组包含两个元素，一个是特征名，一个是该特征的权重值。\n",
    "    words: List[List[str]] -- 一个包含多个样例的列表，每个样例都是一个包含多个词语的列表，表示该样例中出现的所有词语。\n",
    "\n",
    "    Returns:\n",
    "    List[List[str]] -- 一个包含多个样例的列表，每个样例都是一个包含多个异常特征的列表，表示该样例中所有未出现在关注点中的特征。\n",
    "    \"\"\"\n",
    "    errList = []\n",
    "    #遍历每个样例\n",
    "    for imp,con in zip(importance,words):\n",
    "        err = []\n",
    "        for e1 in imp:\n",
    "            #遍历重要特征，判断w是否是关注点，如果不是则异常\n",
    "            w = e1[0].strip()\n",
    "            #遍历关注点\n",
    "            if(any(w in item for item in con)==False) : err.append(w)\n",
    "        errList.append(err)\n",
    "    return errList"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The product shall ensure that only supervisors can view schedule of all callers.The product must ensure that supervisors are allowed to access advertise empty time slots.'\n"
     ]
    },
    {
     "data": {
      "text/plain": "['product', 'that', 'can', 'of', 'product', 'must', 'are', 'to', \"'\"]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errlist = errCheck(importance,words)\n",
    "print(sample[0])\n",
    "print(errlist[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
